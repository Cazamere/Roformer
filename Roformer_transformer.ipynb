{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cazamere/Roformer/blob/main/Roformer_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku4JvtNSDWSm"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxZeYr-UDWSo"
      },
      "source": [
        "Data Sourcing and Processing\n",
        "============================\n",
        "\n",
        "[torchtext library](https://pytorch.org/text/stable/) has utilities for\n",
        "creating datasets that can be easily iterated through for the purposes\n",
        "of creating a language translation model. In this example, we show how\n",
        "to use torchtext\\'s inbuilt datasets, tokenize a raw text sentence,\n",
        "build vocabulary, and numericalize tokens into tensor. We will use\n",
        "[Multi30k dataset from torchtext\n",
        "library](https://pytorch.org/text/stable/datasets.html#multi30k) that\n",
        "yields a pair of source-target raw sentences.\n",
        "\n",
        "To access torchtext datasets, please install torchdata following\n",
        "instructions at <https://github.com/pytorch/data>.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kowZuR4nDWSp"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "\n",
        "\n",
        "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
        "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "#multi30k.URL[\"test\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'en'\n",
        "TGT_LANGUAGE = 'de'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLfKWAHBDWSp"
      },
      "source": [
        "Create source and target language tokenizer. Make sure to install the\n",
        "dependencies.\n",
        "\n",
        "``` {.sourceCode .python}\n",
        "pip install -U torchdata\n",
        "pip install -U spacy\n",
        "python -m spacy download en_core_web_sm\n",
        "python -m spacy download de_core_news_sm\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torchdata\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZibgmEGDgAq",
        "outputId": "2bcc5f09-4434-4cd0-93a0-bc7a8ee05294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata) (1.3.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting de-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install portalocker==2.8.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0f6qwYoEHIi",
        "outputId": "5936cbf6-316c-415a-8fd3-b551c56501bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: portalocker==2.8.2 in /usr/local/lib/python3.10/dist-packages (2.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Xtb8GRnDWSp"
      },
      "outputs": [],
      "source": [
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # Create torchtext's Vocab object\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A9pKyM5zx_J"
      },
      "source": [
        "# New MHA Forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOYtrGu0oJJG"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch import Tensor, _VF, FloatTensor\n",
        "from typing import Callable, List, Optional, Tuple, Union\n",
        "from torch.types import _dtype as DType\n",
        "import warnings\n",
        "\n",
        "def dropout(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:\n",
        "    \"\"\"During training, randomly zeroes some elements of the input tensor with probability :attr:`p`.\n",
        "\n",
        "    Uses samples from a Bernoulli distribution.\n",
        "\n",
        "    See :class:`~torch.nn.Dropout` for details.\n",
        "\n",
        "    Args:\n",
        "        p: probability of an element to be zeroed. Default: 0.5\n",
        "        training: apply dropout if is ``True``. Default: ``True``\n",
        "        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
        "    \"\"\"\n",
        "    #if has_torch_function_unary(input):\n",
        "    #    return handle_torch_function(dropout, (input,), input, p=p, training=training, inplace=inplace)\n",
        "    if p < 0.0 or p > 1.0:\n",
        "        raise ValueError(f\"dropout probability has to be between 0 and 1, but got {p}\")\n",
        "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
        "\n",
        "def _get_softmax_dim(name: str, ndim: int, stacklevel: int) -> int:\n",
        "    #warnings.warn(\n",
        "    #    f\"Implicit dimension choice for {name} has been deprecated. Change the call to include dim=X as an argument.\",\n",
        "    #    stacklevel=stacklevel,\n",
        "    #)\n",
        "    if ndim == 0 or ndim == 1 or ndim == 3:\n",
        "        ret = 0\n",
        "    else:\n",
        "        ret = 1\n",
        "    return ret\n",
        "\n",
        "def softmax(input: Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[DType] = None) -> Tensor:\n",
        "    r\"\"\"Apply a softmax function.\n",
        "\n",
        "    Softmax is defined as:\n",
        "\n",
        "    :math:`\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}`\n",
        "\n",
        "    It is applied to all slices along dim, and will re-scale them so that the elements\n",
        "    lie in the range `[0, 1]` and sum to 1.\n",
        "\n",
        "    See :class:`~torch.nn.Softmax` for more details.\n",
        "\n",
        "    Args:\n",
        "        input (Tensor): input\n",
        "        dim (int): A dimension along which softmax will be computed.\n",
        "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
        "          If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
        "          is performed. This is useful for preventing data type overflows. Default: None.\n",
        "\n",
        "    .. note::\n",
        "        This function doesn't work directly with NLLLoss,\n",
        "        which expects the Log to be computed between the Softmax and itself.\n",
        "        Use log_softmax instead (it's faster and has better numerical properties).\n",
        "\n",
        "    \"\"\"\n",
        "   # if has_torch_function_unary(input):\n",
        "   #     return handle_torch_function(softmax, (input,), input, dim=dim, _stacklevel=_stacklevel, dtype=dtype)\n",
        "    if dim is None:\n",
        "        dim = _get_softmax_dim(\"softmax\", input.dim(), _stacklevel)\n",
        "    if dtype is None:\n",
        "        ret = input.softmax(dim)\n",
        "    else:\n",
        "        ret = input.softmax(dim, dtype=dtype)\n",
        "    return ret\n",
        "\n",
        "def linear(input, weight, bias=None):\n",
        "    # type: (Tensor, Tensor, Optional[Tensor]) -> Tensor\n",
        "    r\"\"\"\n",
        "    Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
        "\n",
        "    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
        "\n",
        "    Shape:\n",
        "\n",
        "        - Input: :math:`(N, *, in\\_features)` N is the batch size, `*` means any number of\n",
        "          additional dimensions\n",
        "        - Weight: :math:`(out\\_features, in\\_features)`\n",
        "        - Bias: :math:`(out\\_features)`\n",
        "        - Output: :math:`(N, *, out\\_features)`\n",
        "    \"\"\"\n",
        "    tens_ops = (input, weight)\n",
        "    \"\"\"\n",
        "    if not torch.jit.is_scripting():\n",
        "        if any([type(t) is not Tensor for t in tens_ops]) and has_torch_function(tens_ops):\n",
        "            return handle_torch_function(linear, tens_ops, input, weight, bias=bias)\n",
        "    \"\"\"\n",
        "    if input.dim() == 2 and bias is not None:\n",
        "        # fused op is marginally faster\n",
        "        ret = torch.addmm(bias, input, weight.t())\n",
        "    else:\n",
        "        output = input.matmul(weight.t())\n",
        "        if bias is not None:\n",
        "            output += bias\n",
        "        ret = output\n",
        "    return ret\n",
        "\n",
        "def pad(input: Tensor, pad: List[int], mode: str = \"constant\", value: Optional[float] = None) -> Tensor:\n",
        "    r\"\"\"\n",
        "pad(input, pad, mode=\"constant\", value=None) -> Tensor\n",
        "\n",
        "Pads tensor.\n",
        "\n",
        "Padding size:\n",
        "    The padding size by which to pad some dimensions of :attr:`input`\n",
        "    are described starting from the last dimension and moving forward.\n",
        "    :math:`\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor` dimensions\n",
        "    of ``input`` will be padded.\n",
        "    For example, to pad only the last dimension of the input tensor, then\n",
        "    :attr:`pad` has the form\n",
        "    :math:`(\\text{padding\\_left}, \\text{padding\\_right})`;\n",
        "    to pad the last 2 dimensions of the input tensor, then use\n",
        "    :math:`(\\text{padding\\_left}, \\text{padding\\_right},`\n",
        "    :math:`\\text{padding\\_top}, \\text{padding\\_bottom})`;\n",
        "    to pad the last 3 dimensions, use\n",
        "    :math:`(\\text{padding\\_left}, \\text{padding\\_right},`\n",
        "    :math:`\\text{padding\\_top}, \\text{padding\\_bottom}`\n",
        "    :math:`\\text{padding\\_front}, \\text{padding\\_back})`.\n",
        "\n",
        "Padding mode:\n",
        "    See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,\n",
        "    :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`\n",
        "    for concrete examples on how each of the padding modes works. Constant\n",
        "    padding is implemented for arbitrary dimensions. Circular, replicate and\n",
        "    reflection padding are implemented for padding the last 3 dimensions of a\n",
        "    4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,\n",
        "    or the last dimension of a 2D or 3D input tensor.\n",
        "\n",
        "Note:\n",
        "    When using the CUDA backend, this operation may induce nondeterministic\n",
        "    behaviour in its backward pass that is not easily switched off.\n",
        "    Please see the notes on :doc:`/notes/randomness` for background.\n",
        "\n",
        "Args:\n",
        "    input (Tensor): N-dimensional tensor\n",
        "    pad (tuple): m-elements tuple, where\n",
        "        :math:`\\frac{m}{2} \\leq` input dimensions and :math:`m` is even.\n",
        "    mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.\n",
        "        Default: ``'constant'``\n",
        "    value: fill value for ``'constant'`` padding. Default: ``0``\n",
        "\n",
        "\"\"\"\n",
        "   # if has_torch_function_unary(input):\n",
        "   #     return handle_torch_function(\n",
        "   #         torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value)\n",
        "   # if not torch.jit.is_scripting():\n",
        "   #     if torch.are_deterministic_algorithms_enabled() and input.is_cuda:\n",
        "   #         if len(pad) == 4 and (input.dim() == 3 or input.dim() == 4) and mode == 'replicate':\n",
        "                # Use slow decomp whose backward will be in terms of index_put.\n",
        "                # importlib is required because the import cannot be top level\n",
        "                # (cycle) and cannot be nested (TS doesn't support)\n",
        "   #             return importlib.import_module('torch._decomp.decompositions').replication_pad2d(\n",
        "   #                 input, pad\n",
        "   #             )\n",
        "    return torch._C._nn.pad(input, pad, mode, value)\n",
        "\n",
        "def has_torch_function(vt: \"torch._dynamo.variables.base.VariableTracker\") -> bool:\n",
        "    from torch._dynamo.variables import UserDefinedObjectVariable\n",
        "    from torch._dynamo.variables.torch_function import TensorWithTFOverrideVariable\n",
        "\n",
        "    return isinstance(vt, TensorWithTFOverrideVariable) or (\n",
        "        isinstance(vt, UserDefinedObjectVariable)\n",
        "        and hasattr(vt.value, \"__torch_function__\")\n",
        "    )\n",
        "\n",
        "\n",
        "def _mha_shape_check(query: Tensor, key: Tensor, value: Tensor,\n",
        "                     key_padding_mask: Optional[Tensor], attn_mask: Optional[Tensor], num_heads: int):\n",
        "    # Verifies the expected shape for `query, `key`, `value`, `key_padding_mask` and `attn_mask`\n",
        "    # and returns if the input is batched or not.\n",
        "    # Raises an error if `query` is not 2-D (unbatched) or 3-D (batched) tensor.\n",
        "\n",
        "    # Shape check.\n",
        "    if query.dim() == 3:\n",
        "        # Batched Inputs\n",
        "        is_batched = True\n",
        "        assert key.dim() == 3 and value.dim() == 3, \\\n",
        "            (\"For batched (3-D) `query`, expected `key` and `value` to be 3-D\"\n",
        "             f\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\")\n",
        "        if key_padding_mask is not None:\n",
        "            assert key_padding_mask.dim() == 2, \\\n",
        "                (\"For batched (3-D) `query`, expected `key_padding_mask` to be `None` or 2-D\"\n",
        "                 f\" but found {key_padding_mask.dim()}-D tensor instead\")\n",
        "        if attn_mask is not None:\n",
        "            assert attn_mask.dim() in (2, 3), \\\n",
        "                (\"For batched (3-D) `query`, expected `attn_mask` to be `None`, 2-D or 3-D\"\n",
        "                 f\" but found {attn_mask.dim()}-D tensor instead\")\n",
        "    elif query.dim() == 2:\n",
        "        # Unbatched Inputs\n",
        "        is_batched = False\n",
        "        assert key.dim() == 2 and value.dim() == 2, \\\n",
        "            (\"For unbatched (2-D) `query`, expected `key` and `value` to be 2-D\"\n",
        "             f\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\")\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            assert key_padding_mask.dim() == 1, \\\n",
        "                (\"For unbatched (2-D) `query`, expected `key_padding_mask` to be `None` or 1-D\"\n",
        "                 f\" but found {key_padding_mask.dim()}-D tensor instead\")\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            assert attn_mask.dim() in (2, 3), \\\n",
        "                (\"For unbatched (2-D) `query`, expected `attn_mask` to be `None`, 2-D or 3-D\"\n",
        "                 f\" but found {attn_mask.dim()}-D tensor instead\")\n",
        "            if attn_mask.dim() == 3:\n",
        "                expected_shape = (num_heads, query.shape[0], key.shape[0])\n",
        "                assert attn_mask.shape == expected_shape, \\\n",
        "                    (f\"Expected `attn_mask` shape to be {expected_shape} but got {attn_mask.shape}\")\n",
        "    else:\n",
        "        raise AssertionError(\n",
        "            f\"query should be unbatched 2D or batched 3D tensor but received {query.dim()}-D query tensor\")\n",
        "\n",
        "    return is_batched\n",
        "\n",
        "def _canonical_mask(\n",
        "        mask: Optional[Tensor],\n",
        "        mask_name: str,\n",
        "        other_type: Optional[DType],\n",
        "        other_name: str,\n",
        "        target_type: DType,\n",
        "        check_other: bool = True,\n",
        ") -> Optional[Tensor]:\n",
        "\n",
        "    if mask is not None:\n",
        "        _mask_dtype = mask.dtype\n",
        "        _mask_is_float = torch.is_floating_point(mask)\n",
        "        if _mask_dtype != torch.bool and not _mask_is_float:\n",
        "            raise AssertionError(\n",
        "                f\"only bool and floating types of {mask_name} are supported\")\n",
        "        if check_other and other_type is not None:\n",
        "            if _mask_dtype != other_type:\n",
        "                warnings.warn(\n",
        "                    f\"Support for mismatched {mask_name} and {other_name} \"\n",
        "                    \"is deprecated. Use same type for both instead.\"\n",
        "                )\n",
        "        if not _mask_is_float:\n",
        "            mask = (\n",
        "                torch.zeros_like(mask, dtype=target_type)\n",
        "                .masked_fill_(mask, float(\"-inf\"))\n",
        "            )\n",
        "    return mask\n",
        "\n",
        "def _none_or_dtype(input: Optional[Tensor]) -> Optional[DType]:\n",
        "    if input is None:\n",
        "        return None\n",
        "    elif isinstance(input, torch.Tensor):\n",
        "        return input.dtype\n",
        "    raise RuntimeError(\"input to _none_or_dtype() must be None or torch.Tensor\")\n",
        "\n",
        "def _in_projection_packed(\n",
        "    q: Tensor,\n",
        "    k: Tensor,\n",
        "    v: Tensor,\n",
        "    w: Tensor,\n",
        "    b: Optional[Tensor] = None,\n",
        ") -> List[Tensor]:\n",
        "    r\"\"\"Perform the in-projection step of the attention operation, using packed weights.\n",
        "\n",
        "    Output is a triple containing projection tensors for query, key and value.\n",
        "\n",
        "    Args:\n",
        "        q, k, v: query, key and value tensors to be projected. For self-attention,\n",
        "            these are typically the same tensor; for encoder-decoder attention,\n",
        "            k and v are typically the same tensor. (We take advantage of these\n",
        "            identities for performance if they are present.) Regardless, q, k and v\n",
        "            must share a common embedding dimension; otherwise their shapes may vary.\n",
        "        w: projection weights for q, k and v, packed into a single tensor. Weights\n",
        "            are packed along dimension 0, in q, k, v order.\n",
        "        b: optional projection biases for q, k and v, packed into a single tensor\n",
        "            in q, k, v order.\n",
        "\n",
        "    Shape:\n",
        "        Inputs:\n",
        "        - q: :math:`(..., E)` where E is the embedding dimension\n",
        "        - k: :math:`(..., E)` where E is the embedding dimension\n",
        "        - v: :math:`(..., E)` where E is the embedding dimension\n",
        "        - w: :math:`(E * 3, E)` where E is the embedding dimension\n",
        "        - b: :math:`E * 3` where E is the embedding dimension\n",
        "\n",
        "        Output:\n",
        "        - in output list :math:`[q', k', v']`, each output tensor will have the\n",
        "            same shape as the corresponding input tensor.\n",
        "    \"\"\"\n",
        "    E = q.size(-1)\n",
        "    if k is v:\n",
        "        if q is k:\n",
        "            # self-attention\n",
        "            proj = linear(q, w, b)\n",
        "            # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\n",
        "            proj = proj.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
        "            return proj[0], proj[1], proj[2]\n",
        "        else:\n",
        "            # encoder-decoder attention\n",
        "            w_q, w_kv = w.split([E, E * 2])\n",
        "            if b is None:\n",
        "                b_q = b_kv = None\n",
        "            else:\n",
        "                b_q, b_kv = b.split([E, E * 2])\n",
        "            q_proj = linear(q, w_q, b_q)\n",
        "            kv_proj = linear(k, w_kv, b_kv)\n",
        "            # reshape to 2, E and not E, 2 is deliberate for better memory coalescing and keeping same order as chunk()\n",
        "            kv_proj = kv_proj.unflatten(-1, (2, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
        "            return (q_proj, kv_proj[0], kv_proj[1])\n",
        "    else:\n",
        "        w_q, w_k, w_v = w.chunk(3)\n",
        "        if b is None:\n",
        "            b_q = b_k = b_v = None\n",
        "        else:\n",
        "            b_q, b_k, b_v = b.chunk(3)\n",
        "        return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n",
        "\n",
        "\n",
        "def _in_projection(\n",
        "    q: Tensor,\n",
        "    k: Tensor,\n",
        "    v: Tensor,\n",
        "    w_q: Tensor,\n",
        "    w_k: Tensor,\n",
        "    w_v: Tensor,\n",
        "    b_q: Optional[Tensor] = None,\n",
        "    b_k: Optional[Tensor] = None,\n",
        "    b_v: Optional[Tensor] = None,\n",
        ") -> Tuple[Tensor, Tensor, Tensor]:\n",
        "    r\"\"\"Perform the in-projection step of the attention operation.\n",
        "\n",
        "    This is simply a triple of linear projections,\n",
        "    with shape constraints on the weights which\n",
        "    ensure embedding dimension uniformity in the projected outputs.\n",
        "    Output is a triple containing projection tensors for query, key and value.\n",
        "\n",
        "    Args:\n",
        "        q, k, v: query, key and value tensors to be projected.\n",
        "        w_q, w_k, w_v: weights for q, k and v, respectively.\n",
        "        b_q, b_k, b_v: optional biases for q, k and v, respectively.\n",
        "\n",
        "    Shape:\n",
        "        Inputs:\n",
        "        - q: :math:`(Qdims..., Eq)` where Eq is the query embedding dimension and Qdims are any\n",
        "            number of leading dimensions.\n",
        "        - k: :math:`(Kdims..., Ek)` where Ek is the key embedding dimension and Kdims are any\n",
        "            number of leading dimensions.\n",
        "        - v: :math:`(Vdims..., Ev)` where Ev is the value embedding dimension and Vdims are any\n",
        "            number of leading dimensions.\n",
        "        - w_q: :math:`(Eq, Eq)`\n",
        "        - w_k: :math:`(Eq, Ek)`\n",
        "        - w_v: :math:`(Eq, Ev)`\n",
        "        - b_q: :math:`(Eq)`\n",
        "        - b_k: :math:`(Eq)`\n",
        "        - b_v: :math:`(Eq)`\n",
        "\n",
        "        Output: in output triple :math:`(q', k', v')`,\n",
        "         - q': :math:`[Qdims..., Eq]`\n",
        "         - k': :math:`[Kdims..., Eq]`\n",
        "         - v': :math:`[Vdims..., Eq]`\n",
        "\n",
        "    \"\"\"\n",
        "    Eq, Ek, Ev = q.size(-1), k.size(-1), v.size(-1)\n",
        "    assert w_q.shape == (Eq, Eq), f\"expecting query weights shape of {(Eq, Eq)}, but got {w_q.shape}\"\n",
        "    assert w_k.shape == (Eq, Ek), f\"expecting key weights shape of {(Eq, Ek)}, but got {w_k.shape}\"\n",
        "    assert w_v.shape == (Eq, Ev), f\"expecting value weights shape of {(Eq, Ev)}, but got {w_v.shape}\"\n",
        "    assert b_q is None or b_q.shape == (Eq,), f\"expecting query bias shape of {(Eq,)}, but got {b_q.shape}\"\n",
        "    assert b_k is None or b_k.shape == (Eq,), f\"expecting key bias shape of {(Eq,)}, but got {b_k.shape}\"\n",
        "    assert b_v is None or b_v.shape == (Eq,), f\"expecting value bias shape of {(Eq,)}, but got {b_v.shape}\"\n",
        "    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n",
        "\n",
        "def new_multi_head_attention_forward(\n",
        "    query: Tensor,\n",
        "    key: Tensor,\n",
        "    value: Tensor,\n",
        "    embed_dim_to_check: int,\n",
        "    num_heads: int,\n",
        "    in_proj_weight: Optional[Tensor],\n",
        "    in_proj_bias: Optional[Tensor],\n",
        "    bias_k: Optional[Tensor],\n",
        "    bias_v: Optional[Tensor],\n",
        "    add_zero_attn: bool,\n",
        "    dropout_p: float,\n",
        "    out_proj_weight: Tensor,\n",
        "    out_proj_bias: Optional[Tensor],\n",
        "    training: bool = True,\n",
        "    key_padding_mask: Optional[Tensor] = None,\n",
        "    need_weights: bool = True,\n",
        "    attn_mask: Optional[Tensor] = None,\n",
        "    use_separate_proj_weight: bool = False,\n",
        "    q_proj_weight: Optional[Tensor] = None,\n",
        "    k_proj_weight: Optional[Tensor] = None,\n",
        "    v_proj_weight: Optional[Tensor] = None,\n",
        "    static_k: Optional[Tensor] = None,\n",
        "    static_v: Optional[Tensor] = None,\n",
        "    average_attn_weights: bool = True,\n",
        "    is_causal: bool = False,\n",
        ") -> Tuple[Tensor, Optional[Tensor]]:\n",
        "\n",
        "    tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v, out_proj_weight, out_proj_bias)\n",
        "    \"\"\"\n",
        "    if has_torch_function(tens_ops):\n",
        "        return handle_torch_function(\n",
        "            multi_head_attention_forward,\n",
        "            tens_ops,\n",
        "            query,\n",
        "            key,\n",
        "            value,\n",
        "            embed_dim_to_check,\n",
        "            num_heads,\n",
        "            in_proj_weight,\n",
        "            in_proj_bias,\n",
        "            bias_k,\n",
        "            bias_v,\n",
        "            add_zero_attn,\n",
        "            dropout_p,\n",
        "            out_proj_weight,\n",
        "            out_proj_bias,\n",
        "            training=training,\n",
        "            key_padding_mask=key_padding_mask,\n",
        "            need_weights=need_weights,\n",
        "            attn_mask=attn_mask,\n",
        "            is_causal=is_causal,\n",
        "            use_separate_proj_weight=use_separate_proj_weight,\n",
        "            q_proj_weight=q_proj_weight,\n",
        "            k_proj_weight=k_proj_weight,\n",
        "            v_proj_weight=v_proj_weight,\n",
        "            static_k=static_k,\n",
        "            static_v=static_v,\n",
        "            average_attn_weights=average_attn_weights,\n",
        "        )\n",
        "    \"\"\"\n",
        "\n",
        "    is_batched = _mha_shape_check(query, key, value, key_padding_mask, attn_mask, num_heads)\n",
        "\n",
        "    # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input\n",
        "    # is batched, run the computation and before returning squeeze the\n",
        "    # batch dimension so that the output doesn't carry this temporary batch dimension.\n",
        "    if not is_batched:\n",
        "        # unsqueeze if the input is unbatched\n",
        "        query = query.unsqueeze(1)\n",
        "        key = key.unsqueeze(1)\n",
        "        value = value.unsqueeze(1)\n",
        "        if key_padding_mask is not None:\n",
        "            key_padding_mask = key_padding_mask.unsqueeze(0)\n",
        "\n",
        "    # set up shape vars\n",
        "    tgt_len, bsz, embed_dim = query.shape\n",
        "    src_len, _, _ = key.shape\n",
        "\n",
        "    key_padding_mask = _canonical_mask(\n",
        "        mask=key_padding_mask,\n",
        "        mask_name=\"key_padding_mask\",\n",
        "        other_type=_none_or_dtype(attn_mask),\n",
        "        other_name=\"attn_mask\",\n",
        "        target_type=query.dtype\n",
        "    )\n",
        "\n",
        "    if is_causal and attn_mask is None:\n",
        "        raise RuntimeError(\n",
        "            \"Need attn_mask if specifying the is_causal hint. \"\n",
        "            \"You may use the Transformer module method \"\n",
        "            \"`generate_square_subsequent_mask` to create this mask.\"\n",
        "        )\n",
        "\n",
        "    if is_causal and key_padding_mask is None and not need_weights:\n",
        "        # when we have a kpm or need weights, we need attn_mask\n",
        "        # Otherwise, we use the is_causal hint go as is_causal\n",
        "        # indicator to SDPA.\n",
        "        attn_mask = None\n",
        "    else:\n",
        "        attn_mask = _canonical_mask(\n",
        "            mask=attn_mask,\n",
        "            mask_name=\"attn_mask\",\n",
        "            other_type=None,\n",
        "            other_name=\"\",\n",
        "            target_type=query.dtype,\n",
        "            check_other=False,\n",
        "        )\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            # We have the attn_mask, and use that to merge kpm into it.\n",
        "            # Turn off use of is_causal hint, as the merged mask is no\n",
        "            # longer causal.\n",
        "            is_causal = False\n",
        "\n",
        "    assert embed_dim == embed_dim_to_check, \\\n",
        "        f\"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}\"\n",
        "    if isinstance(embed_dim, torch.Tensor):\n",
        "        # embed_dim can be a tensor when JIT tracing\n",
        "        head_dim = embed_dim.div(num_heads, rounding_mode='trunc')\n",
        "    else:\n",
        "        head_dim = embed_dim // num_heads\n",
        "    assert head_dim * num_heads == embed_dim, f\"embed_dim {embed_dim} not divisible by num_heads {num_heads}\"\n",
        "    if use_separate_proj_weight:\n",
        "        # allow MHA to have different embedding dimensions when separate projection weights are used\n",
        "        assert key.shape[:2] == value.shape[:2], \\\n",
        "            f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\"\n",
        "    else:\n",
        "        assert key.shape == value.shape, f\"key shape {key.shape} does not match value shape {value.shape}\"\n",
        "\n",
        "    #\n",
        "    # compute in-projection\n",
        "    #\n",
        "    if not use_separate_proj_weight:\n",
        "        assert in_proj_weight is not None, \"use_separate_proj_weight is False but in_proj_weight is None\"\n",
        "        q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
        "    else:\n",
        "        assert q_proj_weight is not None, \"use_separate_proj_weight is True but q_proj_weight is None\"\n",
        "        assert k_proj_weight is not None, \"use_separate_proj_weight is True but k_proj_weight is None\"\n",
        "        assert v_proj_weight is not None, \"use_separate_proj_weight is True but v_proj_weight is None\"\n",
        "        if in_proj_bias is None:\n",
        "            b_q = b_k = b_v = None\n",
        "        else:\n",
        "            b_q, b_k, b_v = in_proj_bias.chunk(3)\n",
        "        q, k, v = _in_projection(query, key, value, q_proj_weight, k_proj_weight, v_proj_weight, b_q, b_k, b_v)\n",
        "\n",
        "    # prep attention mask\n",
        "\n",
        "    if attn_mask is not None:\n",
        "        # ensure attn_mask's dim is 3\n",
        "        if attn_mask.dim() == 2:\n",
        "            correct_2d_size = (tgt_len, src_len)\n",
        "            if attn_mask.shape != correct_2d_size:\n",
        "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
        "            attn_mask = attn_mask.unsqueeze(0)\n",
        "        elif attn_mask.dim() == 3:\n",
        "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
        "            if attn_mask.shape != correct_3d_size:\n",
        "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
        "        else:\n",
        "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
        "\n",
        "    # add bias along batch dimension (currently second)\n",
        "    if bias_k is not None and bias_v is not None:\n",
        "        assert static_k is None, \"bias cannot be added to static key.\"\n",
        "        assert static_v is None, \"bias cannot be added to static value.\"\n",
        "        k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
        "        v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = pad(attn_mask, (0, 1))\n",
        "        if key_padding_mask is not None:\n",
        "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
        "    else:\n",
        "        assert bias_k is None\n",
        "        assert bias_v is None\n",
        "\n",
        "    #\n",
        "    # reshape q, k, v for multihead attention and make them batch first\n",
        "    #\n",
        "    q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    if static_k is None:\n",
        "        k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    else:\n",
        "        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n",
        "        assert static_k.size(0) == bsz * num_heads, \\\n",
        "            f\"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}\"\n",
        "        assert static_k.size(2) == head_dim, \\\n",
        "            f\"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}\"\n",
        "        k = static_k\n",
        "    if static_v is None:\n",
        "        v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    else:\n",
        "        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n",
        "        assert static_v.size(0) == bsz * num_heads, \\\n",
        "            f\"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}\"\n",
        "        assert static_v.size(2) == head_dim, \\\n",
        "            f\"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}\"\n",
        "        v = static_v\n",
        "\n",
        "    # add zero attention along batch dimension (now first)\n",
        "    if add_zero_attn:\n",
        "        zero_attn_shape = (bsz * num_heads, 1, head_dim)\n",
        "        k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1)\n",
        "        v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1)\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = pad(attn_mask, (0, 1))\n",
        "        if key_padding_mask is not None:\n",
        "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
        "\n",
        "    # update source sequence length after adjustments\n",
        "    src_len = k.size(1)\n",
        "\n",
        "    # merge key padding and attention masks\n",
        "    if key_padding_mask is not None:\n",
        "        assert key_padding_mask.shape == (bsz, src_len), \\\n",
        "            f\"expecting key_padding_mask shape of {(bsz, src_len)}, but got {key_padding_mask.shape}\"\n",
        "        key_padding_mask = key_padding_mask.view(bsz, 1, 1, src_len).   \\\n",
        "            expand(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len)\n",
        "        if attn_mask is None:\n",
        "            attn_mask = key_padding_mask\n",
        "        else:\n",
        "            attn_mask = attn_mask + key_padding_mask\n",
        "\n",
        "    # adjust dropout probability\n",
        "    if not training:\n",
        "        dropout_p = 0.0\n",
        "\n",
        "    #\n",
        "    # (deep breath) calculate attention and out projection\n",
        "    #\n",
        "\n",
        "    if need_weights:\n",
        "        B, Nt, E = q.shape\n",
        "        q_scaled = q * math.sqrt(1.0 / float(E))\n",
        "\n",
        "        assert not (is_causal and attn_mask is None), \"FIXME: is_causal not implemented for need_weights\"\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_output_weights = torch.baddbmm(attn_mask, q_scaled, k.transpose(-2, -1))\n",
        "        else:\n",
        "            attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))\n",
        "        attn_output_weights = softmax(attn_output_weights, dim=-1)\n",
        "        if dropout_p > 0.0:\n",
        "            attn_output_weights = dropout(attn_output_weights, p=dropout_p)\n",
        "\n",
        "        attn_output = torch.bmm(attn_output_weights, v)\n",
        "\n",
        "        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
        "        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
        "        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
        "\n",
        "        # optionally average attention weights over heads\n",
        "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
        "        if average_attn_weights:\n",
        "            attn_output_weights = attn_output_weights.mean(dim=1)\n",
        "\n",
        "        if not is_batched:\n",
        "            # squeeze the output if input was unbatched\n",
        "            attn_output = attn_output.squeeze(1)\n",
        "            attn_output_weights = attn_output_weights.squeeze(0)\n",
        "        return attn_output, attn_output_weights\n",
        "    else:\n",
        "        # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
        "        # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
        "        # in order to match the input for SDPA of (N, num_heads, L, S)\n",
        "        if attn_mask is not None:\n",
        "            if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
        "                attn_mask = attn_mask.unsqueeze(0)\n",
        "            else:\n",
        "                attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
        "\n",
        "        q = q.view(bsz, num_heads, tgt_len, head_dim)\n",
        "        k = k.view(bsz, num_heads, src_len, head_dim)\n",
        "        v = v.view(bsz, num_heads, src_len, head_dim)\n",
        "\n",
        "        attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
        "        attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
        "\n",
        "        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
        "        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
        "        if not is_batched:\n",
        "            # squeeze the output if input was unbatched\n",
        "            attn_output = attn_output.squeeze(1)\n",
        "        return attn_output, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCO7Cv8Hm2qE"
      },
      "source": [
        "# New MHA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kjai_l8lm4eo"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.nn.modules.linear import NonDynamicallyQuantizableLinear\n",
        "from torch.nn.init import constant_, xavier_normal_, xavier_uniform_\n",
        "from torch.nn import Parameter\n",
        "\n",
        "def _check_arg_device(x: Optional[torch.Tensor]) -> bool:\n",
        "    if x is not None:\n",
        "        return x.device.type in [\"cpu\", \"cuda\", torch.utils.backend_registration._privateuse1_backend_name]\n",
        "    return True\n",
        "\n",
        "\n",
        "def _arg_requires_grad(x: Optional[torch.Tensor]) -> bool:\n",
        "    if x is not None:\n",
        "        return x.requires_grad\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_make_fx_tracing():\n",
        "    if not torch.jit.is_scripting():\n",
        "        torch_dispatch_mode_stack = torch.utils._python_dispatch._get_current_dispatch_mode_stack()\n",
        "        return any(type(x) == torch.fx.experimental.proxy_tensor.ProxyTorchDispatchMode for x in torch_dispatch_mode_stack)\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "class NewMultiheadAttention(nn.Module):\n",
        "\n",
        "    __constants__ = ['batch_first']\n",
        "    bias_k: Optional[torch.Tensor]\n",
        "    bias_v: Optional[torch.Tensor]\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,\n",
        "                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:\n",
        "        if embed_dim <= 0 or num_heads <= 0:\n",
        "            raise ValueError(\n",
        "                f\"embed_dim and num_heads must be greater than 0,\"\n",
        "                f\" got embed_dim={embed_dim} and num_heads={num_heads} instead\"\n",
        "            )\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.batch_first = batch_first\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        if not self._qkv_same_embed_dim:\n",
        "            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\n",
        "            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\n",
        "            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\n",
        "            self.register_parameter('in_proj_weight', None)\n",
        "        else:\n",
        "            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))\n",
        "            self.register_parameter('q_proj_weight', None)\n",
        "            self.register_parameter('k_proj_weight', None)\n",
        "            self.register_parameter('v_proj_weight', None)\n",
        "\n",
        "        if bias:\n",
        "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n",
        "        else:\n",
        "            self.register_parameter('in_proj_bias', None)\n",
        "        self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
        "            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        if self._qkv_same_embed_dim:\n",
        "            xavier_uniform_(self.in_proj_weight)\n",
        "        else:\n",
        "            xavier_uniform_(self.q_proj_weight)\n",
        "            xavier_uniform_(self.k_proj_weight)\n",
        "            xavier_uniform_(self.v_proj_weight)\n",
        "\n",
        "        if self.in_proj_bias is not None:\n",
        "            constant_(self.in_proj_bias, 0.)\n",
        "            constant_(self.out_proj.bias, 0.)\n",
        "        if self.bias_k is not None:\n",
        "            xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            xavier_normal_(self.bias_v)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
        "        if '_qkv_same_embed_dim' not in state:\n",
        "            state['_qkv_same_embed_dim'] = True\n",
        "\n",
        "        super().__setstate__(state)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            query: Tensor,\n",
        "            key: Tensor,\n",
        "            value: Tensor,\n",
        "            key_padding_mask: Optional[Tensor] = None,\n",
        "            need_weights: bool = True,\n",
        "            attn_mask: Optional[Tensor] = None,\n",
        "            average_attn_weights: bool = True,\n",
        "            is_causal : bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n",
        "\n",
        "        why_not_fast_path = ''\n",
        "        if ((attn_mask is not None and torch.is_floating_point(attn_mask))\n",
        "           or (key_padding_mask is not None) and torch.is_floating_point(key_padding_mask)):\n",
        "            why_not_fast_path = \"floating-point masks are not supported for fast path.\"\n",
        "\n",
        "        is_batched = query.dim() == 3\n",
        "\n",
        "        key_padding_mask = F._canonical_mask(\n",
        "            mask=key_padding_mask,\n",
        "            mask_name=\"key_padding_mask\",\n",
        "            other_type=F._none_or_dtype(attn_mask),\n",
        "            other_name=\"attn_mask\",\n",
        "            target_type=query.dtype\n",
        "        )\n",
        "\n",
        "        attn_mask = F._canonical_mask(\n",
        "            mask=attn_mask,\n",
        "            mask_name=\"attn_mask\",\n",
        "            other_type=None,\n",
        "            other_name=\"\",\n",
        "            target_type=query.dtype,\n",
        "            check_other=False,\n",
        "        )\n",
        "\n",
        "\n",
        "        if not is_batched:\n",
        "            why_not_fast_path = f\"input not batched; expected query.dim() of 3 but got {query.dim()}\"\n",
        "        elif query is not key or key is not value:\n",
        "            # When lifting this restriction, don't forget to either\n",
        "            # enforce that the dtypes all match or test cases where\n",
        "            # they don't!\n",
        "            why_not_fast_path = \"non-self attention was used (query, key, and value are not the same Tensor)\"\n",
        "        elif self.in_proj_bias is not None and query.dtype != self.in_proj_bias.dtype:\n",
        "            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_bias ({self.in_proj_bias.dtype}) don't match\"\n",
        "        elif self.in_proj_weight is None:\n",
        "            why_not_fast_path = \"in_proj_weight was None\"\n",
        "        elif query.dtype != self.in_proj_weight.dtype:\n",
        "            # this case will fail anyway, but at least they'll get a useful error message.\n",
        "            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_weight ({self.in_proj_weight.dtype}) don't match\"\n",
        "        elif self.training:\n",
        "            why_not_fast_path = \"training is enabled\"\n",
        "        elif (self.num_heads % 2) != 0:\n",
        "            why_not_fast_path = \"self.num_heads is not even\"\n",
        "        elif not self.batch_first:\n",
        "            why_not_fast_path = \"batch_first was not True\"\n",
        "        elif self.bias_k is not None:\n",
        "            why_not_fast_path = \"self.bias_k was not None\"\n",
        "        elif self.bias_v is not None:\n",
        "            why_not_fast_path = \"self.bias_v was not None\"\n",
        "        elif self.add_zero_attn:\n",
        "            why_not_fast_path = \"add_zero_attn was enabled\"\n",
        "        elif not self._qkv_same_embed_dim:\n",
        "            why_not_fast_path = \"_qkv_same_embed_dim was not True\"\n",
        "        elif query.is_nested and (key_padding_mask is not None or attn_mask is not None):\n",
        "            why_not_fast_path = \"supplying both src_key_padding_mask and src_mask at the same time \\\n",
        "                                 is not supported with NestedTensor input\"\n",
        "        elif torch.is_autocast_enabled():\n",
        "            why_not_fast_path = \"autocast is enabled\"\n",
        "\n",
        "        if not why_not_fast_path:\n",
        "            tensor_args = (\n",
        "                query,\n",
        "                key,\n",
        "                value,\n",
        "                self.in_proj_weight,\n",
        "                self.in_proj_bias,\n",
        "                self.out_proj.weight,\n",
        "                self.out_proj.bias,\n",
        "            )\n",
        "            # We have to use list comprehensions below because TorchScript does not support\n",
        "            # generator expressions.\n",
        "            if torch.overrides.has_torch_function(tensor_args):\n",
        "                why_not_fast_path = \"some Tensor argument has_torch_function\"\n",
        "            elif _is_make_fx_tracing():\n",
        "                why_not_fast_path = \"we are running make_fx tracing\"\n",
        "            elif not all(_check_arg_device(x) for x in tensor_args):\n",
        "                why_not_fast_path = (\"some Tensor argument's device is neither one of \"\n",
        "                                     f\"cpu, cuda or {torch.utils.backend_registration._privateuse1_backend_name}\")\n",
        "            elif torch.is_grad_enabled() and any(_arg_requires_grad(x) for x in tensor_args):\n",
        "                why_not_fast_path = (\"grad is enabled and at least one of query or the \"\n",
        "                                     \"input/output projection weights or biases requires_grad\")\n",
        "            if not why_not_fast_path:\n",
        "                merged_mask, mask_type = self.merge_masks(attn_mask, key_padding_mask, query)\n",
        "\n",
        "                if self.in_proj_bias is not None and self.in_proj_weight is not None:\n",
        "                    return torch._native_multi_head_attention(\n",
        "                        query,\n",
        "                        key,\n",
        "                        value,\n",
        "                        self.embed_dim,\n",
        "                        self.num_heads,\n",
        "                        self.in_proj_weight,\n",
        "                        self.in_proj_bias,\n",
        "                        self.out_proj.weight,\n",
        "                        self.out_proj.bias,\n",
        "                        merged_mask,\n",
        "                        need_weights,\n",
        "                        average_attn_weights,\n",
        "                        mask_type)\n",
        "\n",
        "        any_nested = query.is_nested or key.is_nested or value.is_nested\n",
        "        assert not any_nested, (\"MultiheadAttention does not support NestedTensor outside of its fast path. \" +\n",
        "                                f\"The fast path was not hit because {why_not_fast_path}\")\n",
        "\n",
        "        if self.batch_first and is_batched:\n",
        "            # make sure that the transpose op does not affect the \"is\" property\n",
        "            if key is value:\n",
        "                if query is key:\n",
        "                    query = key = value = query.transpose(1, 0)\n",
        "                else:\n",
        "                    query, key = (x.transpose(1, 0) for x in (query, key))\n",
        "                    value = key\n",
        "            else:\n",
        "                query, key, value = (x.transpose(1, 0) for x in (query, key, value))\n",
        "\n",
        "        if not self._qkv_same_embed_dim:\n",
        "            attn_output, attn_output_weights = new_multi_head_attention_forward(\n",
        "                query, key, value, self.embed_dim, self.num_heads,\n",
        "                self.in_proj_weight, self.in_proj_bias,\n",
        "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
        "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                attn_mask=attn_mask,\n",
        "                use_separate_proj_weight=True,\n",
        "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
        "                v_proj_weight=self.v_proj_weight,\n",
        "                average_attn_weights=average_attn_weights,\n",
        "                is_causal=is_causal)\n",
        "        else:\n",
        "            attn_output, attn_output_weights = new_multi_head_attention_forward(\n",
        "                query, key, value, self.embed_dim, self.num_heads,\n",
        "                self.in_proj_weight, self.in_proj_bias,\n",
        "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
        "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask,\n",
        "                need_weights=need_weights,\n",
        "                attn_mask=attn_mask,\n",
        "                average_attn_weights=average_attn_weights,\n",
        "                is_causal=is_causal)\n",
        "        if self.batch_first and is_batched:\n",
        "            return attn_output.transpose(1, 0), attn_output_weights\n",
        "        else:\n",
        "            return attn_output, attn_output_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf9oAv0wLFpa"
      },
      "source": [
        "# Roformer MHA Forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaxm-RXtLFpj"
      },
      "outputs": [],
      "source": [
        "def roformer_multi_head_attention_forward(\n",
        "    query: Tensor,\n",
        "    key: Tensor,\n",
        "    value: Tensor,\n",
        "    embed_dim_to_check: int,\n",
        "    num_heads: int,\n",
        "    in_proj_weight: Optional[Tensor],\n",
        "    in_proj_bias: Optional[Tensor],\n",
        "    bias_k: Optional[Tensor],\n",
        "    bias_v: Optional[Tensor],\n",
        "    add_zero_attn: bool,\n",
        "    dropout_p: float,\n",
        "    out_proj_weight: Tensor,\n",
        "    out_proj_bias: Optional[Tensor],\n",
        "    training: bool = True,\n",
        "    key_padding_mask: Optional[Tensor] = None,\n",
        "    need_weights: bool = True,\n",
        "    attn_mask: Optional[Tensor] = None,\n",
        "    use_separate_proj_weight: bool = False,\n",
        "    q_proj_weight: Optional[Tensor] = None,\n",
        "    k_proj_weight: Optional[Tensor] = None,\n",
        "    v_proj_weight: Optional[Tensor] = None,\n",
        "    static_k: Optional[Tensor] = None,\n",
        "    static_v: Optional[Tensor] = None,\n",
        "    average_attn_weights: bool = True,\n",
        "    is_causal: bool = False,\n",
        ") -> Tuple[Tensor, Optional[Tensor]]:\n",
        "\n",
        "    tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v, out_proj_weight, out_proj_bias)\n",
        "    \"\"\"\n",
        "    if has_torch_function(tens_ops):\n",
        "        return handle_torch_function(\n",
        "            multi_head_attention_forward,\n",
        "            tens_ops,\n",
        "            query,\n",
        "            key,\n",
        "            value,\n",
        "            embed_dim_to_check,\n",
        "            num_heads,\n",
        "            in_proj_weight,\n",
        "            in_proj_bias,\n",
        "            bias_k,\n",
        "            bias_v,\n",
        "            add_zero_attn,\n",
        "            dropout_p,\n",
        "            out_proj_weight,\n",
        "            out_proj_bias,\n",
        "            training=training,\n",
        "            key_padding_mask=key_padding_mask,\n",
        "            need_weights=need_weights,\n",
        "            attn_mask=attn_mask,\n",
        "            is_causal=is_causal,\n",
        "            use_separate_proj_weight=use_separate_proj_weight,\n",
        "            q_proj_weight=q_proj_weight,\n",
        "            k_proj_weight=k_proj_weight,\n",
        "            v_proj_weight=v_proj_weight,\n",
        "            static_k=static_k,\n",
        "            static_v=static_v,\n",
        "            average_attn_weights=average_attn_weights,\n",
        "        )\n",
        "    \"\"\"\n",
        "\n",
        "    is_batched = _mha_shape_check(query, key, value, key_padding_mask, attn_mask, num_heads)\n",
        "\n",
        "    # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input\n",
        "    # is batched, run the computation and before returning squeeze the\n",
        "    # batch dimension so that the output doesn't carry this temporary batch dimension.\n",
        "    if not is_batched:\n",
        "        # unsqueeze if the input is unbatched\n",
        "        query = query.unsqueeze(1)\n",
        "        key = key.unsqueeze(1)\n",
        "        value = value.unsqueeze(1)\n",
        "        if key_padding_mask is not None:\n",
        "            key_padding_mask = key_padding_mask.unsqueeze(0)\n",
        "\n",
        "    # set up shape vars\n",
        "    tgt_len, bsz, embed_dim = query.shape\n",
        "    src_len, _, _ = key.shape\n",
        "\n",
        "    key_padding_mask = _canonical_mask(\n",
        "        mask=key_padding_mask,\n",
        "        mask_name=\"key_padding_mask\",\n",
        "        other_type=_none_or_dtype(attn_mask),\n",
        "        other_name=\"attn_mask\",\n",
        "        target_type=query.dtype\n",
        "    )\n",
        "\n",
        "    if is_causal and attn_mask is None:\n",
        "        raise RuntimeError(\n",
        "            \"Need attn_mask if specifying the is_causal hint. \"\n",
        "            \"You may use the Transformer module method \"\n",
        "            \"`generate_square_subsequent_mask` to create this mask.\"\n",
        "        )\n",
        "\n",
        "    if is_causal and key_padding_mask is None and not need_weights:\n",
        "        # when we have a kpm or need weights, we need attn_mask\n",
        "        # Otherwise, we use the is_causal hint go as is_causal\n",
        "        # indicator to SDPA.\n",
        "        attn_mask = None\n",
        "    else:\n",
        "        attn_mask = _canonical_mask(\n",
        "            mask=attn_mask,\n",
        "            mask_name=\"attn_mask\",\n",
        "            other_type=None,\n",
        "            other_name=\"\",\n",
        "            target_type=query.dtype,\n",
        "            check_other=False,\n",
        "        )\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            # We have the attn_mask, and use that to merge kpm into it.\n",
        "            # Turn off use of is_causal hint, as the merged mask is no\n",
        "            # longer causal.\n",
        "            is_causal = False\n",
        "\n",
        "    assert embed_dim == embed_dim_to_check, \\\n",
        "        f\"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}\"\n",
        "    if isinstance(embed_dim, torch.Tensor):\n",
        "        # embed_dim can be a tensor when JIT tracing\n",
        "        head_dim = embed_dim.div(num_heads, rounding_mode='trunc')\n",
        "    else:\n",
        "        head_dim = embed_dim // num_heads\n",
        "    assert head_dim * num_heads == embed_dim, f\"embed_dim {embed_dim} not divisible by num_heads {num_heads}\"\n",
        "    if use_separate_proj_weight:\n",
        "        # allow MHA to have different embedding dimensions when separate projection weights are used\n",
        "        assert key.shape[:2] == value.shape[:2], \\\n",
        "            f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\"\n",
        "    else:\n",
        "        assert key.shape == value.shape, f\"key shape {key.shape} does not match value shape {value.shape}\"\n",
        "\n",
        "    #\n",
        "    # compute in-projection\n",
        "    #\n",
        "    if not use_separate_proj_weight:\n",
        "        assert in_proj_weight is not None, \"use_separate_proj_weight is False but in_proj_weight is None\"\n",
        "        q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
        "    else:\n",
        "        assert q_proj_weight is not None, \"use_separate_proj_weight is True but q_proj_weight is None\"\n",
        "        assert k_proj_weight is not None, \"use_separate_proj_weight is True but k_proj_weight is None\"\n",
        "        assert v_proj_weight is not None, \"use_separate_proj_weight is True but v_proj_weight is None\"\n",
        "        if in_proj_bias is None:\n",
        "            b_q = b_k = b_v = None\n",
        "        else:\n",
        "            b_q, b_k, b_v = in_proj_bias.chunk(3)\n",
        "        q, k, v = _in_projection(query, key, value, q_proj_weight, k_proj_weight, v_proj_weight, b_q, b_k, b_v)\n",
        "\n",
        "    # prep attention mask\n",
        "\n",
        "    if attn_mask is not None:\n",
        "        # ensure attn_mask's dim is 3\n",
        "        if attn_mask.dim() == 2:\n",
        "            correct_2d_size = (tgt_len, src_len)\n",
        "            if attn_mask.shape != correct_2d_size:\n",
        "                raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
        "            attn_mask = attn_mask.unsqueeze(0)\n",
        "        elif attn_mask.dim() == 3:\n",
        "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
        "            if attn_mask.shape != correct_3d_size:\n",
        "                raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
        "        else:\n",
        "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
        "\n",
        "    # add bias along batch dimension (currently second)\n",
        "    if bias_k is not None and bias_v is not None:\n",
        "        assert static_k is None, \"bias cannot be added to static key.\"\n",
        "        assert static_v is None, \"bias cannot be added to static value.\"\n",
        "        k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
        "        v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = pad(attn_mask, (0, 1))\n",
        "        if key_padding_mask is not None:\n",
        "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
        "    else:\n",
        "        assert bias_k is None\n",
        "        assert bias_v is None\n",
        "\n",
        "    #\n",
        "    # reshape q, k, v for multihead attention and make them batch first\n",
        "    #\n",
        "    q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    if static_k is None:\n",
        "        k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    else:\n",
        "        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n",
        "        assert static_k.size(0) == bsz * num_heads, \\\n",
        "            f\"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}\"\n",
        "        assert static_k.size(2) == head_dim, \\\n",
        "            f\"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}\"\n",
        "        k = static_k\n",
        "    if static_v is None:\n",
        "        v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    else:\n",
        "        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n",
        "        assert static_v.size(0) == bsz * num_heads, \\\n",
        "            f\"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}\"\n",
        "        assert static_v.size(2) == head_dim, \\\n",
        "            f\"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}\"\n",
        "        v = static_v\n",
        "\n",
        "    # add zero attention along batch dimension (now first)\n",
        "    if add_zero_attn:\n",
        "        zero_attn_shape = (bsz * num_heads, 1, head_dim)\n",
        "        k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1)\n",
        "        v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1)\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = pad(attn_mask, (0, 1))\n",
        "        if key_padding_mask is not None:\n",
        "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
        "\n",
        "    # update source sequence length after adjustments\n",
        "    src_len = k.size(1)\n",
        "\n",
        "    # merge key padding and attention masks\n",
        "    if key_padding_mask is not None:\n",
        "        assert key_padding_mask.shape == (bsz, src_len), \\\n",
        "            f\"expecting key_padding_mask shape of {(bsz, src_len)}, but got {key_padding_mask.shape}\"\n",
        "        key_padding_mask = key_padding_mask.view(bsz, 1, 1, src_len).   \\\n",
        "            expand(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len)\n",
        "        if attn_mask is None:\n",
        "            attn_mask = key_padding_mask\n",
        "        else:\n",
        "            attn_mask = attn_mask + key_padding_mask\n",
        "\n",
        "    # adjust dropout probability\n",
        "    if not training:\n",
        "        dropout_p = 0.0\n",
        "\n",
        "    #\n",
        "    # (deep breath) calculate attention and out projection\n",
        "    #\n",
        "\n",
        "    if need_weights:\n",
        "        B, Nt, E = q.shape\n",
        "        q_scaled = q * math.sqrt(1.0 / float(E))\n",
        "\n",
        "        assert not (is_causal and attn_mask is None), \"FIXME: is_causal not implemented for need_weights\"\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_output_weights = torch.baddbmm(attn_mask, q_scaled, k.transpose(-2, -1))\n",
        "        else:\n",
        "            attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))\n",
        "        attn_output_weights = softmax(attn_output_weights, dim=-1)\n",
        "        if dropout_p > 0.0:\n",
        "            attn_output_weights = dropout(attn_output_weights, p=dropout_p)\n",
        "\n",
        "        attn_output = torch.bmm(attn_output_weights, v)\n",
        "\n",
        "        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
        "        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
        "        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
        "\n",
        "        # optionally average attention weights over heads\n",
        "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
        "        if average_attn_weights:\n",
        "            attn_output_weights = attn_output_weights.mean(dim=1)\n",
        "\n",
        "        if not is_batched:\n",
        "            # squeeze the output if input was unbatched\n",
        "            attn_output = attn_output.squeeze(1)\n",
        "            attn_output_weights = attn_output_weights.squeeze(0)\n",
        "        return attn_output, attn_output_weights\n",
        "    else:\n",
        "        # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
        "        # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
        "        # in order to match the input for SDPA of (N, num_heads, L, S)\n",
        "        if attn_mask is not None:\n",
        "            if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
        "                attn_mask = attn_mask.unsqueeze(0)\n",
        "            else:\n",
        "                attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
        "\n",
        "        q = q.view(bsz, num_heads, tgt_len, head_dim)\n",
        "        k = k.view(bsz, num_heads, src_len, head_dim)\n",
        "        v = v.view(bsz, num_heads, src_len, head_dim)\n",
        "\n",
        "        attn_output = torch._C._nn.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
        "        attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
        "\n",
        "        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
        "        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
        "        if not is_batched:\n",
        "            # squeeze the output if input was unbatched\n",
        "            attn_output = attn_output.squeeze(1)\n",
        "        return attn_output, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUo4IisjLBNR"
      },
      "source": [
        "# Roformer MHA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKpJAVnYLBNa"
      },
      "outputs": [],
      "source": [
        "class RoformerMultiheadAttention(nn.Module):\n",
        "\n",
        "    __constants__ = ['batch_first']\n",
        "    bias_k: Optional[torch.Tensor]\n",
        "    bias_v: Optional[torch.Tensor]\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,\n",
        "                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:\n",
        "        if embed_dim <= 0 or num_heads <= 0:\n",
        "            raise ValueError(\n",
        "                f\"embed_dim and num_heads must be greater than 0,\"\n",
        "                f\" got embed_dim={embed_dim} and num_heads={num_heads} instead\"\n",
        "            )\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.batch_first = batch_first\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        if not self._qkv_same_embed_dim:\n",
        "            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\n",
        "            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\n",
        "            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\n",
        "            self.register_parameter('in_proj_weight', None)\n",
        "        else:\n",
        "            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))\n",
        "            self.register_parameter('q_proj_weight', None)\n",
        "            self.register_parameter('k_proj_weight', None)\n",
        "            self.register_parameter('v_proj_weight', None)\n",
        "\n",
        "        if bias:\n",
        "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n",
        "        else:\n",
        "            self.register_parameter('in_proj_bias', None)\n",
        "        self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
        "            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        if self._qkv_same_embed_dim:\n",
        "            xavier_uniform_(self.in_proj_weight)\n",
        "        else:\n",
        "            xavier_uniform_(self.q_proj_weight)\n",
        "            xavier_uniform_(self.k_proj_weight)\n",
        "            xavier_uniform_(self.v_proj_weight)\n",
        "\n",
        "        if self.in_proj_bias is not None:\n",
        "            constant_(self.in_proj_bias, 0.)\n",
        "            constant_(self.out_proj.bias, 0.)\n",
        "        if self.bias_k is not None:\n",
        "            xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            xavier_normal_(self.bias_v)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
        "        if '_qkv_same_embed_dim' not in state:\n",
        "            state['_qkv_same_embed_dim'] = True\n",
        "\n",
        "        super().__setstate__(state)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            query: Tensor,\n",
        "            key: Tensor,\n",
        "            value: Tensor,\n",
        "            key_padding_mask: Optional[Tensor] = None,\n",
        "            need_weights: bool = True,\n",
        "            attn_mask: Optional[Tensor] = None,\n",
        "            average_attn_weights: bool = True,\n",
        "            is_causal : bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n",
        "\n",
        "        why_not_fast_path = ''\n",
        "        if ((attn_mask is not None and torch.is_floating_point(attn_mask))\n",
        "           or (key_padding_mask is not None) and torch.is_floating_point(key_padding_mask)):\n",
        "            why_not_fast_path = \"floating-point masks are not supported for fast path.\"\n",
        "\n",
        "        is_batched = query.dim() == 3\n",
        "\n",
        "        key_padding_mask = F._canonical_mask(\n",
        "            mask=key_padding_mask,\n",
        "            mask_name=\"key_padding_mask\",\n",
        "            other_type=F._none_or_dtype(attn_mask),\n",
        "            other_name=\"attn_mask\",\n",
        "            target_type=query.dtype\n",
        "        )\n",
        "\n",
        "        attn_mask = F._canonical_mask(\n",
        "            mask=attn_mask,\n",
        "            mask_name=\"attn_mask\",\n",
        "            other_type=None,\n",
        "            other_name=\"\",\n",
        "            target_type=query.dtype,\n",
        "            check_other=False,\n",
        "        )\n",
        "\n",
        "\n",
        "        if not is_batched:\n",
        "            why_not_fast_path = f\"input not batched; expected query.dim() of 3 but got {query.dim()}\"\n",
        "        elif query is not key or key is not value:\n",
        "            # When lifting this restriction, don't forget to either\n",
        "            # enforce that the dtypes all match or test cases where\n",
        "            # they don't!\n",
        "            why_not_fast_path = \"non-self attention was used (query, key, and value are not the same Tensor)\"\n",
        "        elif self.in_proj_bias is not None and query.dtype != self.in_proj_bias.dtype:\n",
        "            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_bias ({self.in_proj_bias.dtype}) don't match\"\n",
        "        elif self.in_proj_weight is None:\n",
        "            why_not_fast_path = \"in_proj_weight was None\"\n",
        "        elif query.dtype != self.in_proj_weight.dtype:\n",
        "            # this case will fail anyway, but at least they'll get a useful error message.\n",
        "            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_weight ({self.in_proj_weight.dtype}) don't match\"\n",
        "        elif self.training:\n",
        "            why_not_fast_path = \"training is enabled\"\n",
        "        elif (self.num_heads % 2) != 0:\n",
        "            why_not_fast_path = \"self.num_heads is not even\"\n",
        "        elif not self.batch_first:\n",
        "            why_not_fast_path = \"batch_first was not True\"\n",
        "        elif self.bias_k is not None:\n",
        "            why_not_fast_path = \"self.bias_k was not None\"\n",
        "        elif self.bias_v is not None:\n",
        "            why_not_fast_path = \"self.bias_v was not None\"\n",
        "        elif self.add_zero_attn:\n",
        "            why_not_fast_path = \"add_zero_attn was enabled\"\n",
        "        elif not self._qkv_same_embed_dim:\n",
        "            why_not_fast_path = \"_qkv_same_embed_dim was not True\"\n",
        "        elif query.is_nested and (key_padding_mask is not None or attn_mask is not None):\n",
        "            why_not_fast_path = \"supplying both src_key_padding_mask and src_mask at the same time \\\n",
        "                                 is not supported with NestedTensor input\"\n",
        "        elif torch.is_autocast_enabled():\n",
        "            why_not_fast_path = \"autocast is enabled\"\n",
        "\n",
        "        if not why_not_fast_path:\n",
        "            tensor_args = (\n",
        "                query,\n",
        "                key,\n",
        "                value,\n",
        "                self.in_proj_weight,\n",
        "                self.in_proj_bias,\n",
        "                self.out_proj.weight,\n",
        "                self.out_proj.bias,\n",
        "            )\n",
        "            # We have to use list comprehensions below because TorchScript does not support\n",
        "            # generator expressions.\n",
        "            if torch.overrides.has_torch_function(tensor_args):\n",
        "                why_not_fast_path = \"some Tensor argument has_torch_function\"\n",
        "            elif _is_make_fx_tracing():\n",
        "                why_not_fast_path = \"we are running make_fx tracing\"\n",
        "            elif not all(_check_arg_device(x) for x in tensor_args):\n",
        "                why_not_fast_path = (\"some Tensor argument's device is neither one of \"\n",
        "                                     f\"cpu, cuda or {torch.utils.backend_registration._privateuse1_backend_name}\")\n",
        "            elif torch.is_grad_enabled() and any(_arg_requires_grad(x) for x in tensor_args):\n",
        "                why_not_fast_path = (\"grad is enabled and at least one of query or the \"\n",
        "                                     \"input/output projection weights or biases requires_grad\")\n",
        "            if not why_not_fast_path:\n",
        "                merged_mask, mask_type = self.merge_masks(attn_mask, key_padding_mask, query)\n",
        "\n",
        "                if self.in_proj_bias is not None and self.in_proj_weight is not None:\n",
        "                    return torch._native_multi_head_attention(\n",
        "                        query,\n",
        "                        key,\n",
        "                        value,\n",
        "                        self.embed_dim,\n",
        "                        self.num_heads,\n",
        "                        self.in_proj_weight,\n",
        "                        self.in_proj_bias,\n",
        "                        self.out_proj.weight,\n",
        "                        self.out_proj.bias,\n",
        "                        merged_mask,\n",
        "                        need_weights,\n",
        "                        average_attn_weights,\n",
        "                        mask_type)\n",
        "\n",
        "        any_nested = query.is_nested or key.is_nested or value.is_nested\n",
        "        assert not any_nested, (\"MultiheadAttention does not support NestedTensor outside of its fast path. \" +\n",
        "                                f\"The fast path was not hit because {why_not_fast_path}\")\n",
        "\n",
        "        if self.batch_first and is_batched:\n",
        "            # make sure that the transpose op does not affect the \"is\" property\n",
        "            if key is value:\n",
        "                if query is key:\n",
        "                    query = key = value = query.transpose(1, 0)\n",
        "                else:\n",
        "                    query, key = (x.transpose(1, 0) for x in (query, key))\n",
        "                    value = key\n",
        "            else:\n",
        "                query, key, value = (x.transpose(1, 0) for x in (query, key, value))\n",
        "\n",
        "        if not self._qkv_same_embed_dim:\n",
        "            attn_output, attn_output_weights = roformer_multi_head_attention_forward(\n",
        "                query, key, value, self.embed_dim, self.num_heads,\n",
        "                self.in_proj_weight, self.in_proj_bias,\n",
        "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
        "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
        "                attn_mask=attn_mask,\n",
        "                use_separate_proj_weight=True,\n",
        "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
        "                v_proj_weight=self.v_proj_weight,\n",
        "                average_attn_weights=average_attn_weights,\n",
        "                is_causal=is_causal)\n",
        "        else:\n",
        "            attn_output, attn_output_weights = roformer_multi_head_attention_forward(\n",
        "                query, key, value, self.embed_dim, self.num_heads,\n",
        "                self.in_proj_weight, self.in_proj_bias,\n",
        "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
        "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask,\n",
        "                need_weights=need_weights,\n",
        "                attn_mask=attn_mask,\n",
        "                average_attn_weights=average_attn_weights,\n",
        "                is_causal=is_causal)\n",
        "        if self.batch_first and is_batched:\n",
        "            return attn_output.transpose(1, 0), attn_output_weights\n",
        "        else:\n",
        "            return attn_output, attn_output_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMTSsKnEDWSp"
      },
      "source": [
        "Seq2Seq Network using Transformer\n",
        "=================================\n",
        "\n",
        "Transformer is a Seq2Seq model introduced in [\"Attention is all you\n",
        "need\"](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
        "paper for solving machine translation tasks. Below, we will create a\n",
        "Seq2Seq network that uses Transformer. The network consists of three\n",
        "parts. First part is the embedding layer. This layer converts tensor of\n",
        "input indices into corresponding tensor of input embeddings. These\n",
        "embedding are further augmented with positional encodings to provide\n",
        "position information of input tokens to the model. The second part is\n",
        "the actual\n",
        "[Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)\n",
        "model. Finally, the output of the Transformer model is passed through\n",
        "linear layer that gives unnormalized probabilities for each token in the\n",
        "target language.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgoidqsTDWSq"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\"\"\"\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size: int):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, emb_size: int, nhead: int, dim_feedforward: int, dropout: float):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = NewMultiheadAttention(emb_size, nhead, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(emb_size, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, emb_size)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor, src_key_padding_mask: Tensor):\n",
        "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, emb_size: int, nhead: int, dim_feedforward: int, dropout: float):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.self_attn = NewMultiheadAttention(emb_size, nhead, dropout=dropout)\n",
        "        self.multihead_attn = NewMultiheadAttention(emb_size, nhead, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(emb_size, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, emb_size)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "        self.norm3 = nn.LayerNorm(emb_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor, memory_mask: Tensor,\n",
        "                tgt_key_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, emb_size, nhead, num_layers, dim_feedforward, dropout):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(emb_size, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, src, mask, src_key_padding_mask):\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        src = self.norm(src)\n",
        "        return src\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, emb_size, nhead, num_layers, dim_feedforward, dropout):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([TransformerDecoderLayer(emb_size, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask):\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
        "                        tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "        tgt = self.norm(tgt)\n",
        "        return tgt\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, emb_size, nhead, src_vocab_size, tgt_vocab_size, dim_feedforward=512, dropout=0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout)\n",
        "\n",
        "        self.encoder = TransformerEncoder(emb_size, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
        "        self.decoder = TransformerDecoder(emb_size, nhead, num_decoder_layers, dim_feedforward, dropout)\n",
        "\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        #print(f'{src_emb.shape=}')\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
        "        #print(f'{tgt_emb.shape=}')\n",
        "        #print(ojerhfgor)\n",
        "        memory = self.encoder(src_emb, src_mask, src_padding_mask)\n",
        "        outs = self.decoder(tgt_emb, memory, tgt_mask, None, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        return self.encoder(src_emb, src_mask, None)\n",
        "\n",
        "    def decode(self, tgt, memory, tgt_mask):\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
        "        return self.decoder(tgt_emb, memory, tgt_mask, None, None, None)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GkbidGAK-oBC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "44350897-cdc6-4739-e5a9-48924b2f5870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nclass PositionalEncoding(nn.Module):\\n    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\\n        super(PositionalEncoding, self).__init__()\\n        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\\n        pos_embedding = torch.zeros((maxlen, emb_size))\\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\\n        pos_embedding = pos_embedding.unsqueeze(-2)\\n\\n        self.dropout = nn.Dropout(dropout)\\n        self.register_buffer('pos_embedding', pos_embedding)\\n\\n    def forward(self, token_embedding: Tensor):\\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\\n\\nclass TokenEmbedding(nn.Module):\\n    def __init__(self, vocab_size: int, emb_size: int):\\n        super(TokenEmbedding, self).__init__()\\n        self.embedding = nn.Embedding(vocab_size, emb_size)\\n        self.emb_size = emb_size\\n\\n    def forward(self, tokens: Tensor):\\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\\n\\nclass TransformerEncoderLayer(nn.Module):\\n    def __init__(self, emb_size: int, nhead: int, dim_feedforward: int, dropout: float):\\n        super(TransformerEncoderLayer, self).__init__()\\n        self.self_attn = NewMultiheadAttention(emb_size, nhead, dropout=dropout)\\n        self.linear1 = nn.Linear(emb_size, dim_feedforward)\\n        self.dropout = nn.Dropout(dropout)\\n        self.linear2 = nn.Linear(dim_feedforward, emb_size)\\n\\n        self.norm1 = nn.LayerNorm(emb_size)\\n        self.norm2 = nn.LayerNorm(emb_size)\\n        self.dropout1 = nn.Dropout(dropout)\\n        self.dropout2 = nn.Dropout(dropout)\\n\\n        self.activation = nn.ReLU()\\n\\n    def forward(self, src: Tensor, src_mask: Tensor, src_key_padding_mask: Tensor):\\n        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\\n                              key_padding_mask=src_key_padding_mask)[0]\\n        src = src + self.dropout1(src2)\\n        src = self.norm1(src)\\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\\n        src = src + self.dropout2(src2)\\n        src = self.norm2(src)\\n        return src\\n\\nclass TransformerDecoderLayer(nn.Module):\\n    def __init__(self, emb_size: int, nhead: int, dim_feedforward: int, dropout: float):\\n        super(TransformerDecoderLayer, self).__init__()\\n        self.self_attn = NewMultiheadAttention(emb_size, nhead, dropout=dropout)\\n        self.multihead_attn = NewMultiheadAttention(emb_size, nhead, dropout=dropout)\\n        self.linear1 = nn.Linear(emb_size, dim_feedforward)\\n        self.dropout = nn.Dropout(dropout)\\n        self.linear2 = nn.Linear(dim_feedforward, emb_size)\\n\\n        self.norm1 = nn.LayerNorm(emb_size)\\n        self.norm2 = nn.LayerNorm(emb_size)\\n        self.norm3 = nn.LayerNorm(emb_size)\\n        self.dropout1 = nn.Dropout(dropout)\\n        self.dropout2 = nn.Dropout(dropout)\\n        self.dropout3 = nn.Dropout(dropout)\\n\\n        self.activation = nn.ReLU()\\n\\n    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor, memory_mask: Tensor,\\n                tgt_key_padding_mask: Tensor, memory_key_padding_mask: Tensor):\\n        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\\n                              key_padding_mask=tgt_key_padding_mask)[0]\\n        tgt = tgt + self.dropout1(tgt2)\\n        tgt = self.norm1(tgt)\\n        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\\n                                   key_padding_mask=memory_key_padding_mask)[0]\\n        tgt = tgt + self.dropout2(tgt2)\\n        tgt = self.norm2(tgt)\\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\\n        tgt = tgt + self.dropout3(tgt2)\\n        tgt = self.norm3(tgt)\\n        return tgt\\n\\nclass TransformerEncoder(nn.Module):\\n    def __init__(self, emb_size, nhead, num_layers, dim_feedforward, dropout):\\n        super(TransformerEncoder, self).__init__()\\n        self.layers = nn.ModuleList([TransformerEncoderLayer(emb_size, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\\n        self.norm = nn.LayerNorm(emb_size)\\n\\n    def forward(self, src, mask, src_key_padding_mask):\\n        for layer in self.layers:\\n            src = layer(src, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\\n        src = self.norm(src)\\n        return src\\n\\nclass TransformerDecoder(nn.Module):\\n    def __init__(self, emb_size, nhead, num_layers, dim_feedforward, dropout):\\n        super(TransformerDecoder, self).__init__()\\n        self.layers = nn.ModuleList([TransformerDecoderLayer(emb_size, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\\n        self.norm = nn.LayerNorm(emb_size)\\n\\n    def forward(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask):\\n        for layer in self.layers:\\n            tgt = layer(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\\n                        tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\\n        tgt = self.norm(tgt)\\n        return tgt\\n\\nclass Seq2SeqTransformer(nn.Module):\\n    def __init__(self, num_encoder_layers, num_decoder_layers, emb_size, nhead, src_vocab_size, tgt_vocab_size, dim_feedforward=512, dropout=0.1):\\n        super(Seq2SeqTransformer, self).__init__()\\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\\n        self.positional_encoding = PositionalEncoding(emb_size, dropout)\\n\\n        self.encoder = TransformerEncoder(emb_size, nhead, num_encoder_layers, dim_feedforward, dropout)\\n        self.decoder = TransformerDecoder(emb_size, nhead, num_decoder_layers, dim_feedforward, dropout)\\n\\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\\n\\n    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\\n        src_emb = self.positional_encoding(self.src_tok_emb(src))\\n        #print(f'{src_emb.shape=}')\\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\\n        #print(f'{tgt_emb.shape=}')\\n        #print(ojerhfgor)\\n        memory = self.encoder(src_emb, src_mask, src_padding_mask)\\n        outs = self.decoder(tgt_emb, memory, tgt_mask, None, tgt_padding_mask, memory_key_padding_mask)\\n        return self.generator(outs)\\n\\n    def encode(self, src, src_mask):\\n        src_emb = self.positional_encoding(self.src_tok_emb(src))\\n        return self.encoder(src_emb, src_mask, None)\\n\\n    def decode(self, tgt, memory, tgt_mask):\\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\\n        return self.decoder(tgt_emb, memory, tgt_mask, None, None, None)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Roformer\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim)) # w_i/theta_i\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "\n",
        "    def forward(self, pos): # pos -> p, 27\n",
        "        #print(f'{self.inv_freq.shape=}') # 128\n",
        "        #print(fskdjf)\n",
        "        sinusoid_inp = torch.outer(pos.float().squeeze(0), self.inv_freq) # [m_1 * theta, m_2 * theta, ], (27, 128)\n",
        "        #print(f'{sinusoid_inp.shape=}')\n",
        "        #print(f'{sinusoid_inp.sin().shape=}')\n",
        "        #print(f'{sinusoid_inp.cos().shape=}')\n",
        "        return torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(x, sincos):\n",
        "    #print()\n",
        "    #print(f'{sincos.shape=}')\n",
        "    #print(f'{sincos.unbind(-1)=}')\n",
        "    sin, cos = torch.split(sincos, sincos.shape[-1] // 2, dim=1)#sincos.unbind(-1)\n",
        "    sin = sin.unsqueeze(1)\n",
        "    cos = cos.unsqueeze(1)\n",
        "    #print(f'{sin.shape=}')\n",
        "    #print(f'{cos.shape=}')\n",
        "    #print(f'{x.shape=}')\n",
        "    x1, x2 = x[..., :x.size(-1) // 2], x[..., x.size(-1) // 2:]\n",
        "    #print(f'{x1.shape=}')\n",
        "    #print(f'{x2.shape=}')\n",
        "    #print(f'{(x1 * cos).shape=}')\n",
        "    #print(f'{(x2 * sin).shape=}')\n",
        "    #print(f'{(x2 * cos).shape=}')\n",
        "    #print(f'{(-x1 * sin).shape=}')\n",
        "    return torch.cat([x1 * cos + x2 * sin, x2 * cos - x1 * sin], dim=-1)\n",
        "\n",
        "class RotaryAttention(nn.Module):\n",
        "    def __init__(self, emb_size, nhead, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.multihead_attn = nn.MultiheadAttention(emb_size, nhead, dropout=dropout)\n",
        "        self.rotary_emb = RotaryEmbedding(emb_size) # emb_size // 2\n",
        "\n",
        "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
        "        query_seq_len, key_seq_len, batch_size = query.size(0), key.size(0), query.size(1)\n",
        "        #print(f'{query.size(0)=}')\n",
        "        #print(f'{key.size(0)=}')\n",
        "        query_pos = torch.arange(query_seq_len, device=query.device).unsqueeze(0) # Generates a range of position indices from 0 to seq_len-1\n",
        "        query_pos_emb = self.rotary_emb(query_pos)\n",
        "\n",
        "        key_pos = torch.arange(key_seq_len, device=key.device).unsqueeze(0) # Generates a range of position indices from 0 to seq_len-1\n",
        "        key_pos_emb = self.rotary_emb(key_pos)\n",
        "\n",
        "        # Apply rotary embeddings to queries and keys\n",
        "        query, key = apply_rotary_pos_emb(query, query_pos_emb), apply_rotary_pos_emb(key, key_pos_emb)\n",
        "\n",
        "        return self.multihead_attn(query, key, value, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size: int):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "class RoformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, emb_size: int, nhead: int, dim_feedforward: int, dropout: float):\n",
        "        super(RoformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = RotaryAttention(emb_size, nhead, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(emb_size, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, emb_size)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor, src_key_padding_mask: Tensor):\n",
        "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "class RoformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, emb_size: int, nhead: int, dim_feedforward: int, dropout: float):\n",
        "        super(RoformerDecoderLayer, self).__init__()\n",
        "        self.self_attn = RotaryAttention(emb_size, nhead, dropout=dropout)\n",
        "        self.multihead_attn = RotaryAttention(emb_size, nhead, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(emb_size, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, emb_size)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "        self.norm3 = nn.LayerNorm(emb_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor, memory_mask: Tensor,\n",
        "                tgt_key_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "class RoformerEncoder(nn.Module):\n",
        "    def __init__(self, emb_size, nhead, num_layers, dim_feedforward, dropout):\n",
        "        super(RoformerEncoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([RoformerEncoderLayer(emb_size, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, src, mask, src_key_padding_mask):\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        src = self.norm(src)\n",
        "        return src\n",
        "\n",
        "class RoformerDecoder(nn.Module):\n",
        "    def __init__(self, emb_size, nhead, num_layers, dim_feedforward, dropout):\n",
        "        super(RoformerDecoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([RoformerDecoderLayer(emb_size, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask):\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
        "                        tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "        tgt = self.norm(tgt)\n",
        "        return tgt\n",
        "\n",
        "class Roformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, emb_size, nhead, src_vocab_size, tgt_vocab_size, dim_feedforward=512, dropout=0.1):\n",
        "        super(Roformer, self).__init__()\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        #self.positional_encoding = PositionalEncoding(emb_size, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.encoder = RoformerEncoder(emb_size, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
        "        self.decoder = RoformerDecoder(emb_size, nhead, num_decoder_layers, dim_feedforward, dropout)\n",
        "\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "\n",
        "        #self.freqs_complex = precompute_theta_pos_frequencies(emb_size // nhead, self.args.max_seq_len * 2, device=DEVICE)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
        "        #src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        #tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
        "        src_emb = self.dropout(self.src_tok_emb(src))\n",
        "        tgt_emb = self.dropout(self.tgt_tok_emb(tgt))\n",
        "        #print(f'{src_emb.shape=}')\n",
        "        #print(f'{tgt_emb.shape=}')\n",
        "        #print(ojerhfgor)\n",
        "\n",
        "        memory = self.encoder(src_emb, src_mask, src_padding_mask)\n",
        "        outs = self.decoder(tgt_emb, memory, tgt_mask, None, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        #src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        src_emb = self.dropout(self.src_tok_emb(src))\n",
        "        return self.encoder(src_emb, src_mask, None)\n",
        "\n",
        "    def decode(self, tgt, memory, tgt_mask):\n",
        "        #tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
        "        tgt_emb = self.dropout(self.tgt_tok_emb(tgt))\n",
        "        return self.decoder(tgt_emb, memory, tgt_mask, None, None, None)"
      ],
      "metadata": {
        "id": "9WN6ABe0Fdf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eiY6-xtDWSq"
      },
      "source": [
        "During training, we need a subsequent word mask that will prevent the\n",
        "model from looking into the future words when making predictions. We\n",
        "will also need masks to hide source and target padding tokens. Below,\n",
        "let\\'s define a function that will take care of both.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRz2-zfRDWSq"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IteYw_EnDWSq"
      },
      "source": [
        "Let\\'s now define the parameters of our model and instantiate the same.\n",
        "Below, we also define our loss function which is the cross-entropy loss\n",
        "and the optimizer used for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31PaTU8oDWSq"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import sys\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "model = 'roformer'\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 4\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 2\n",
        "NUM_DECODER_LAYERS = 2\n",
        "\n",
        "if model == 'transformer':\n",
        "  transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "elif model == 'roformer':\n",
        "  transformer = Roformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "else:\n",
        "  sys.exit('Model not supported')\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1)\n",
        "\n",
        "class CustomScheduler(_LRScheduler):\n",
        "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "        super(CustomScheduler, self).__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        step = max(self.last_epoch, 1)\n",
        "        scaling = self.d_model ** -0.5\n",
        "        return [base_lr * scaling * min(step ** -0.5, step * self.warmup_steps ** -1.5) for base_lr in self.base_lrs]\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9) # 1e-7 if using scheduler\n",
        "#scheduler = CustomScheduler(optimizer, EMB_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtvwxHx7DWSq"
      },
      "source": [
        "Collation\n",
        "=========\n",
        "\n",
        "As seen in the `Data Sourcing and Processing` section, our data iterator\n",
        "yields a pair of raw strings. We need to convert these string pairs into\n",
        "the batched tensors that can be processed by our `Seq2Seq` network\n",
        "defined previously. Below we define our collate function that converts a\n",
        "batch of raw strings into batch tensors that can be fed directly into\n",
        "our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dng_yDF0DWSq"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "5xPePWpRFmSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6eNksvsDWSr"
      },
      "source": [
        "Let\\'s define training and evaluation loop that will be called for each\n",
        "epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTLa3JB1DWSr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    #print(len(train_dataloader))\n",
        "    #print(fsui)\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pUZxHShDWSr"
      },
      "source": [
        "Now we have all the ingredients to train our model. Let\\'s do it!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYYj56Y1DWSr",
        "outputId": "b7044947-bdb3-4b7f-dfab-59602bccd3da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [00:26<01:44, 26.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 6.156, Val loss: 4.865, Epoch time = 25.708s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [00:52<01:18, 26.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Train loss: 4.451, Val loss: 4.136, Epoch time = 25.441s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [01:18<00:52, 26.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Train loss: 3.880, Val loss: 3.784, Epoch time = 25.470s\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "for epoch in tqdm(range(1, NUM_EPOCHS+1)):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    #scheduler.step()\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    #bleu = evaluate_test(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate_str(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").split(\" \")[1:-1]"
      ],
      "metadata": {
        "id": "yuzzqWrAF_4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2kdf1NqDWSr"
      },
      "outputs": [],
      "source": [
        "print(translate_str(transformer, \"A group of people stand in front of an Igloo.\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.metrics import bleu_score"
      ],
      "metadata": {
        "id": "6f2m9fChwNhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = bleu_score([translate_str(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\")], [['A', 'group', 'of', 'people', 'stand', 'in', 'front', 'of', 'an', 'igloo', '.']], max_n=1, weights=[1])\n",
        "print(f\"BLEU score: {bleu*100:.2f}\")"
      ],
      "metadata": {
        "id": "qcYB7nNbwdWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_corpus = [['My', 'full', 'pytorch', 'test'], ['Another', 'Sentence']]\n",
        "references_corpus = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']], [['No', 'Match']]]\n",
        "bleu_score(candidate_corpus, references_corpus)"
      ],
      "metadata": {
        "id": "jjEij8dB01JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(model, src, src_mask, max_len, start_symbol, beam_size, alpha):\n",
        "    \"\"\"\n",
        "    Perform beam search for sequence generation.\n",
        "\n",
        "    :param model: The sequence generation model.\n",
        "    :param src_input: The input tensor for source sequence.\n",
        "    :param src_mask: The mask tensor for the source sequence.\n",
        "    :param max_len: Maximum length of the generated sequence.\n",
        "    :param start_symbol: The start symbol id for the sequence.\n",
        "    :param beam_size: Number of beams to keep.\n",
        "    :param alpha: Length penalty parameter.\n",
        "    :return: The best hypothesis sequence.\n",
        "    \"\"\"\n",
        "    # Initializations\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask).to(DEVICE)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "\n",
        "    active_beams = [(0, [start_symbol])]  # Each beam is a tuple(score, token_ids)\n",
        "\n",
        "    for step in range(max_len):\n",
        "        new_beams = []\n",
        "        for score, token_ids in active_beams:\n",
        "            # Assuming `model` has a method `predict_next_token_probabilities`\n",
        "            # that returns the next-token log probabilities and takes the current token sequence.\n",
        "            # You will need to replace this with the actual call to your model.\n",
        "            #probabilities = model.predict_next_token_probabilities(src_input, src_mask, token_ids)\n",
        "\n",
        "            tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                        .type(torch.bool)).to(DEVICE)\n",
        "            out = model.decode(ys, memory, tgt_mask)\n",
        "            out = out.transpose(0, 1)\n",
        "            probabilities = model.generator(out[:, -1])\n",
        "\n",
        "            # Get top `beam_size` next tokens and their log probabilities\n",
        "            top_probs, top_tokens = torch.topk(probabilities, beam_size)\n",
        "            #print(f'{top_probs=}')\n",
        "            #print(f'{top_probs.shape=}')\n",
        "            #print(f'{top_probs.squeeze(0)=}')\n",
        "            #print(f'{top_probs.squeeze(0).shape=}')\n",
        "            #print(f'{top_tokens=}')\n",
        "            top_probs, top_tokens = top_probs.squeeze(0), top_tokens.squeeze(0)\n",
        "\n",
        "            # Add new hypotheses to `new_beams`\n",
        "            for i in range(beam_size):\n",
        "                next_score = score + top_probs[i].item()\n",
        "                next_token_ids = token_ids + [top_tokens[i].item()]\n",
        "                new_beams.append((next_score, next_token_ids))\n",
        "\n",
        "                if next_token_ids[-1] == EOS_IDX:  # Early stopping if end_symbol is generated\n",
        "                    return next_token_ids\n",
        "\n",
        "        # Keep top `beam_size` hypotheses based on scores adjusted by length penalty\n",
        "        active_beams = sorted(new_beams, key=lambda x: x[0] / (len(x[1]) ** alpha), reverse=True)[:beam_size]\n",
        "        #print(f'{active_beams=}')\n",
        "\n",
        "    # Return the hypothesis with the highest score after adjusting by length penalty\n",
        "    return max(active_beams, key=lambda x: x[0] / (len(x[1]) ** alpha))[1]\n"
      ],
      "metadata": {
        "id": "fLDqHQRrsKzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# actual function to translate input sentence into target language\n",
        "def translate_greedy(model: torch.nn.Module, src):\n",
        "    model.eval()\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "\n",
        "    #print(tgt_tokens)\n",
        "    #print(fsdd)\n",
        "\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").split(\" \")[1:-1]\n",
        "\n",
        "def translate_beam_search(model: torch.nn.Module, src, beam_size=4, alpha=0.6):\n",
        "    model.eval()\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = beam_search(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, beam_size=beam_size, alpha=alpha)\n",
        "\n",
        "    #print(tgt_tokens)\n",
        "    #print(fsdd)\n",
        "\n",
        "\n",
        "    #print(f'{tgt_tokens=}')\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(tgt_tokens)).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").split(\" \")[1:-1]\n",
        "\n",
        "def evaluate_test(model, mode):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    candidate_corpus = []\n",
        "    reference_corpus = []\n",
        "\n",
        "    test_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE)) # the test split in PyTorch is broken, so evaluating on valid split\n",
        "    test_dataloader = DataLoader(test_iter, batch_size=1, collate_fn=collate_fn)  # Set batch_size to 1 for simplicity\n",
        "    for src, tgt in tqdm(test_dataloader):\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        print(tgt.shape)\n",
        "        print(sfdsf)\n",
        "\n",
        "        # Translate the source sentence\n",
        "        if mode == 'greedy':\n",
        "          translated_sentence = translate_greedy(model, src)\n",
        "        elif mode == 'beam':\n",
        "          translated_sentence = translate_beam_search(model, src, beam_size=2, alpha=0.6) # Doesn't work great, likely due to small training set\n",
        "        else:\n",
        "            sys.exit('Mode not supported')\n",
        "        candidate_corpus.append(translated_sentence)\n",
        "\n",
        "        # Prepare the reference sentence\n",
        "        tgt_sent = [tok for tok in tgt.view(-1).cpu().numpy()]\n",
        "        tgt_sent = \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(tgt_sent)).replace(\"<bos>\", \"\").replace(\"<eos>\", \".\").split(\" \")[1:]\n",
        "        reference_corpus.append([tgt_sent])\n",
        "\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu = bleu_score(candidate_corpus, reference_corpus)\n",
        "    return bleu"
      ],
      "metadata": {
        "id": "HScOnShQx9r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = evaluate_test(transformer, 'greedy')\n",
        "bleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "L0mZjXF1yAXb",
        "outputId": "6f7d29c2-9489-46c9-de81-036239945eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([11, 1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sfdsf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-ae3669156874>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'greedy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-29eaa3525023>\u001b[0m in \u001b[0;36mevaluate_test\u001b[0;34m(model, mode)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msfdsf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Translate the source sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sfdsf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # epochs     absolute           RoPE\n",
        "#    5          0.227            0.248"
      ],
      "metadata": {
        "id": "X2CtSUDY-fcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Convert from nn.Transformer to encoders, decoders, and multi headed attn source code X\n",
        "2.   Add RoPE to new transformer X\n",
        "2.   Think we need to write a script that takes in all that text, and makes it look like something we can train/evaluate on. Go through the train dataloader and see how a batch looks like?\n",
        "2.   Evaluate on test split, not val split\n",
        "3.   Try and download original WMT: https://pytorch.org/text/_modules/torchtext/datasets/translation.html\n",
        "4.   Find the correct way to calculate bleu values\n",
        "2.   Change everything to decoder-only?\n",
        "\n"
      ],
      "metadata": {
        "id": "X6tAuYAXMRJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8y_-CUtUNAUq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-A9pKyM5zx_J",
        "uCO7Cv8Hm2qE",
        "wf9oAv0wLFpa",
        "wUo4IisjLBNR"
      ],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}